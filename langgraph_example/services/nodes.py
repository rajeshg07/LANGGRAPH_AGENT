
import os
import logging
from dotenv import load_dotenv
from openai import OpenAI
import serpapi
from langgraph_example.schemas.state import AgentState

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

class Node:
    @staticmethod
    async def intent_detector(state: AgentState) -> AgentState:
        query = state["query"].lower()
        state["logs"].append(f"Detecting intent for query: {query}")
        
        openai_api_key = os.getenv("OPENAI_API_KEY")
        client = OpenAI(api_key=openai_api_key) if openai_api_key else None

        if client:
            try:
                # Define a prompt to classify intent
                prompt = f"""
                Classify the intent of the following query into one of these categories: 'hashtags', 'trends', or 'general'.
                Provide only the category as a single word in lowercase.
                Query: {query}
                """
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}]
                )
                intent = response.choices[0].message.content.strip().lower()
                # Validate the intent is one of the expected categories
                if intent not in ["hashtags", "trends", "general"]:
                    intent = "general"  # Fallback if LLM returns unexpected output
                state["intent"] = intent
                state["logs"].append(f"Intent detected by LLM: {state['intent']}")
            except Exception as e:
                logger.error(f"LLM intent detection failed: {e}")
                state["intent"] = "general"  # Fallback to general intent on error
                state["logs"].append(f"LLM intent detection failed, falling back to 'general'")
        else:
            state["intent"] = "general"  # Fallback if no API key
            state["logs"].append("LLM not available: No API key, falling back to 'general'")

        state["retry_count"] = state.get("retry_count", 0)
        state["is_valid"] = False  # Reset validation status
        return state

    @staticmethod
    async def hashtag_generator(state: AgentState) -> AgentState:
        query = state["query"].lower()
        openai_api_key = os.getenv("OPENAI_API_KEY")
        serpapi_api_key = os.getenv("SERPAPI_API_KEY")
        client = OpenAI(api_key=openai_api_key) if openai_api_key else None

        if state["intent"] == "hashtags":
            hashtags = []
            platform = "instagram" if "instagram" in query else "tiktok" if "tiktok" in query else "social media"

            if openai_api_key and serpapi_api_key:
                search = serpapi.Client(api_key=serpapi_api_key)
                try:
                    # Try SerpApi first for real-time data
                    results = search.search({"q": f"trending {platform} hashtags", "tbm": "isch"})
                    hashtags = results.get("hashtags", [])[:10]
                    state["logs"].append("Hashtags fetched from SerpApi")
                except Exception as e:
                    logger.error(f"SerpApi hashtag fetch failed: {e}")
                    state["logs"].append("SerpApi hashtag fetch failed, attempting LLM generation")

            # If SerpApi fails or no API key, use LLM to generate hashtags
            if not hashtags and client:
                try:
                    prompt = f"""
                    Generate 10 trending hashtags for a {platform} post based on this query: "{query}".
                    Return the hashtags as a comma-separated list, each starting with #.
                    Example: #hashtag1, #hashtag2, #hashtag3
                    """
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}]
                    )
                    hashtag_text = response.choices[0].message.content.strip()
                    hashtags = [h.strip() for h in hashtag_text.split(",") if h.strip().startswith("#")][:10]
                    state["logs"].append("Hashtags generated by LLM")
                except Exception as e:
                    logger.error(f"LLM hashtag generation failed: {e}")
                    state["logs"].append("LLM hashtag generation failed")

            # Final fallback if both SerpApi and LLM fail
            if not hashtags:
                state["response"] = f"Sorry, I couldn't generate hashtags for your {platform} post due to an error. Please try again later!"
                state["hashtags"] = []
                state["flags"] = {"hashtag_error": True}
                state["logs"].append("Failed to generate hashtags")
            else:
                state["hashtags"] = hashtags
                state["response"] = f"Hey there! Here are some hashtags for your {platform} post: {', '.join(hashtags)}. Hope they help boost your post!"
                state["flags"] = {"hashtag_generated": True}
                state["logs"].append("Hashtags generated successfully")
        return state

    @staticmethod
    async def trend_analyzer(state: AgentState) -> AgentState:
        query = state["query"].lower()
        openai_api_key = os.getenv("OPENAI_API_KEY")
        serpapi_api_key = os.getenv("SERPAPI_API_KEY")
        client = OpenAI(api_key=openai_api_key) if openai_api_key else None

        if state["intent"] == "trends":
            trends = []
            platform = "social media"

            if openai_api_key and serpapi_api_key:
                search = serpapi.Client(api_key=serpapi_api_key)
                try:
                    # Try SerpApi first for real-time trends
                    results = search.search({"q": f"trending {platform} topics", "tbm": "nws"})
                    trends = [item["title"] for item in results.get("news_results", [])[:5]]
                    state["logs"].append("Trends fetched from SerpApi")
                except Exception as e:
                    logger.error(f"SerpApi trend fetch failed: {e}")
                    state["logs"].append("SerpApi trend fetch failed, attempting LLM generation")

            # If SerpApi fails or no API key, use LLM to generate trends
            if not trends and client:
                try:
                    prompt = f"""
                    Identify 5 current trends in {platform} based on this query: "{query}".
                    Return the trends as a comma-separated list.
                    Example: trend 1, trend 2, trend 3
                    """
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}]
                    )
                    trend_text = response.choices[0].message.content.strip()
                    trends = [t.strip() for t in trend_text.split(",") if t.strip()][:5]
                    state["logs"].append("Trends generated by LLM")
                except Exception as e:
                    logger.error(f"LLM trend generation failed: {e}")
                    state["logs"].append("LLM trend generation failed")

            # Final fallback if both SerpApi and LLM fail
            if not trends:
                state["response"] = f"Sorry, I couldn't fetch the latest trends in {platform} due to an error. Please try again later!"
                state["trends"] = []
                state["flags"] = {"trend_error": True}
                state["logs"].append("Failed to analyze trends")
            else:
                state["trends"] = trends
                state["response"] = f"Hey! Here are the latest trends in {platform}: {', '.join(trends)}. Let me know how I can assist further!"
                state["flags"] = {"trend_analyzed": True}
                state["logs"].append("Trends analyzed successfully")
        return state

    @staticmethod
    async def llm_handler(state: AgentState) -> AgentState:
        if state["intent"] == "general":
            openai_api_key = os.getenv("OPENAI_API_KEY")
            client = OpenAI(api_key=openai_api_key) if openai_api_key else None

            if client:
                try:
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": state["query"]}]
                    )
                    state["response"] = response.choices[0].message.content
                    state["flags"] = {"llm_handled": True}
                    state["logs"].append("LLM response generated successfully")
                except Exception as e:
                    logger.error(f"LLM handling failed: {e}")
                    state["response"] = "Sorry, I encountered an error processing your request. How else can I assist you?"
                    state["flags"] = {"llm_error": True}
                    state["logs"].append("LLM handling failed")
            else:
                state["response"] = "Currently, I don't have specific data for your request. How else can I assist you?"
                state["flags"] = {"llm_no_api": True}
                state["logs"].append("LLM not available: No API key")
        return state

    @staticmethod
    async def response_validator(state: AgentState) -> AgentState:
        # Check if the response is satisfactory
        if not state["response"] or "error" in state["response"].lower() or len(state["response"]) < 10:
            state["is_valid"] = False
            state["retry_count"] = state.get("retry_count", 0) + 1
            state["logs"].append(f"Response validation failed: {state['response']}")
            # Refine the query for retry
            state["query"] = f"Please try again: {state['query']}"
        else:
            state["is_valid"] = True
            state["logs"].append("Response validated successfully")

        # Limit retries to avoid infinite loops
        if state["retry_count"] >= 3:
            state["is_valid"] = True
            state["response"] = "I couldn't process your request after multiple attempts. How else can I assist you?"
            state["logs"].append("Max retries reached")
        return state

intent_detector = Node().intent_detector
hashtag_generator = Node().hashtag_generator
trend_analyzer = Node().trend_analyzer
llm_handler = Node().llm_handler
response_validator = Node().response_validator